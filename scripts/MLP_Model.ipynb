{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdOpDYgj2WD4iMB00jJxoA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lrnFVQCQc76i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ccc60c54-2311-4ead-e379-bc0e2a465e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pandas==2.2.0\n",
            "  Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting model2vec\n",
            "  Downloading model2vec-0.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.0) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting mlflow-skinny==2.22.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.8)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.40)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.2.0)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading databricks_sdk-0.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting fastapi<1 (from mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.11.4)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.5.3)\n",
            "Collecting uvicorn<1 (from mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from model2vec) (0.5.3)\n",
            "Requirement already satisfied: tokenizers>=0.20 in /usr/local/lib/python3.11/dist-packages (from model2vec) (0.21.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from model2vec) (4.67.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.20->model2vec) (0.31.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (2.38.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20->model2vec) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20->model2vec) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.4.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.22.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.6.1)\n",
            "Downloading pandas-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Downloading model2vec-0.5.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.53.0-py3-none-any.whl (700 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
            "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, numpy, importlib_metadata, gunicorn, graphql-core, deprecated, starlette, pandas, opentelemetry-api, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, graphene, fastapi, databricks-sdk, scikeras, opentelemetry-sdk, model2vec, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alembic-1.15.2 databricks-sdk-0.53.0 deprecated-1.2.18 docker-7.1.0 fastapi-0.115.12 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 importlib_metadata-8.6.1 mlflow-2.22.0 mlflow-skinny-2.22.0 model2vec-0.5.0 numpy-1.26.4 opentelemetry-api-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 pandas-2.2.0 scikeras-0.13.0 starlette-0.46.2 uvicorn-0.34.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "3ae51e8857024b7c87267b50232cc088"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow mlflow scikit-learn pyarrow numpy tensorflow keras scikeras numpy pandas==2.2.0 model2vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold  # Adicionado StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder  # Importado LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import time\n",
        "from sklearn.metrics import make_scorer #importa o make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# 1. Carregar e preparar os dados\n",
        "print(\"Carregando dados...\")\n",
        "df_train = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet')\n",
        "\n",
        "if 'target' not in df_train.columns:\n",
        "    df_train['target'] = df_train['sentiment']\n",
        "\n",
        "# Codificar labels\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "y = df_train['target'].map(sentiment_mapping)\n",
        "X = df_train['comment_cleaned']\n",
        "\n",
        "# Split dos dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=8000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words='english'\n",
        ")\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Converter para arrays densos\n",
        "X_train_vec = X_train_vec.toarray()\n",
        "X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2, 0.3],\n",
        "    'learning_rate': [0.001, 0.0001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [20],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None, l2(0.01)]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores = []\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J_EwtnKAeBd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melhores parâmetros encontrados:\n",
        "units1: 128\n",
        "units2: 32\n",
        "dropout_rate: 0.3\n",
        "learning_rate: 0.001\n",
        "optimizer: adam\n",
        "batch_size: 32\n",
        "epochs: 20\n",
        "activation: relu\n",
        "regularizer: None\n",
        "\n",
        "Melhor score de validação: 0.7561"
      ],
      "metadata": {
        "id": "vD1SJfTNrMm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSXD7XjovgmS",
        "outputId": "5722def0-d191-45bb-91aa-d23cfd3e19fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0MESbi84WKm",
        "outputId": "2abdedc8-72b9-471d-a4fa-7a17f6cc85ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold  # Adicionado StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder  # Importado LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import time\n",
        "from sklearn.metrics import make_scorer #importa o make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import spacy\n",
        "\n",
        "# 1. Carregar e preparar os dados\n",
        "print(\"Carregando dados...\")\n",
        "df_train = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet')\n",
        "\n",
        "if 'target' not in df_train.columns:\n",
        "    df_train['target'] = df_train['sentiment']\n",
        "\n",
        "# Codificar labels\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "y = df_train['target'].map(sentiment_mapping)\n",
        "X = df_train['comment_cleaned']\n",
        "\n",
        "# Split dos dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "\n",
        "# Carregar um modelo do SpaCy (escolha um que inclua embeddings)\n",
        "nlp = spacy.load(\"en_core_web_lg\")  # ou \"en_core_web_lg\" para embeddings maiores\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.vector\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [20],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores = []\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gz-I58jrGhe",
        "outputId": "0ba482b2-62f1-453a-c6f1-a0ad2f8b005d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dados...\n",
            "Vetorizando textos...\n",
            "\n",
            "Configurando Grid Search...\n",
            "\n",
            "Iniciando Grid Search...\n",
            "\n",
            "Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Recall (Negativo): 0.7391\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "Recall (Negativo): 0.7857\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step \n",
            "Recall (Negativo): 0.9571\n",
            "Recall (Negativo) médio (validação cruzada): 0.8273\n",
            "------------------------------\n",
            "\n",
            "Tempo total de execução: 0.15 minutos\n",
            "\n",
            "Melhores parâmetros encontrados:\n",
            "units1: 128\n",
            "units2: 32\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.001\n",
            "optimizer: adam\n",
            "batch_size: 32\n",
            "epochs: 20\n",
            "activation: relu\n",
            "regularizer: None\n",
            "\n",
            "Melhor score de validação: 0.8273\n",
            "\n",
            "Avaliando no conjunto de teste...\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\n",
            "Acurácia no teste: 0.8095\n",
            "F1-Score no teste: 0.8103\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.77      0.85      0.81        52\n",
            "     Neutral       0.93      0.77      0.84        35\n",
            "    Positive       0.79      0.80      0.79        60\n",
            "\n",
            "    accuracy                           0.81       147\n",
            "   macro avg       0.83      0.81      0.81       147\n",
            "weighted avg       0.82      0.81      0.81       147\n",
            "\n",
            "\n",
            "Métricas para classe Negative:\n",
            "Acurácia para Negative: 0.8462\n",
            "F1-Score para Negative: 0.9167\n",
            "\n",
            "Exemplos de falsos negativos:\n",
            "1. the food is very average . . . the thai fusion stuff is a bit too sweet , every thing they serve is too sweet here .;\n",
            "2. i know , you were too busy showing off your vintage tee shirt and looking bored , but my agenda is i ' m here to eat and enjoy the company of friends , seeking a pleasant experience .;\n",
            "3. first it took us a long time to find the place .;\n",
            "4. service isnot exactly five star , but thats not really a big deal .;\n",
            "5. the staff is no nonsense .;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Melhores parâmetros encontrados:\n",
        "units1: 128\n",
        "units2: 32\n",
        "dropout_rate: 0.2\n",
        "learning_rate: 0.001\n",
        "optimizer: adam\n",
        "batch_size: 32\n",
        "epochs: 20\n",
        "activation: relu\n",
        "regularizer: None\n",
        "\n",
        "Melhor score de validação: 0.8130"
      ],
      "metadata": {
        "id": "026bomlmwg0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from model2vec import StaticModel\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class Model2VecEmbeddings():\n",
        "    \"\"\"Wrapper para o Model2Vec como Embeddings do LangChain\"\"\"\n",
        "    def __init__(self, model_name: str = \"minishlab/potion-base-2M\", similarity_threshold: float = 0.85):\n",
        "        self.model = StaticModel.from_pretrained(model_name)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.model.encode([text]).tolist()[0]"
      ],
      "metadata": {
        "id": "wotZEcTJ8w1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model2vec import StaticModel\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "class Model2VecEmbeddings():\n",
        "    \"\"\"Wrapper para o Model2Vec como Embeddings do LangChain\"\"\"\n",
        "    def __init__(self, model_name: str = \"minishlab/potion-base-2M\", similarity_threshold: float = 0.85):\n",
        "        self.model = StaticModel.from_pretrained(model_name)\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.model.encode([text]).tolist()[0]\n",
        "\n",
        "def get_model2vec_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Gera o embedding de uma única frase usando Model2VecEmbeddings e retorna um array NumPy.\n",
        "\n",
        "    Args:\n",
        "        text: A frase para a qual gerar o embedding.\n",
        "\n",
        "    Returns:\n",
        "        Um array NumPy representando o vetor de embedding da frase.\n",
        "    \"\"\"\n",
        "    emb = Model2VecEmbeddings()\n",
        "    vector_list = emb.embed_query(text=text)  # Obtém o embedding como uma lista\n",
        "    vector_np = np.array(vector_list)       # Converte a lista para um array NumPy\n",
        "    return vector_np\n",
        "\n",
        "# Testando a função modificada\n",
        "frase_teste_amor = \"I love it!\"\n",
        "embedding_amor = get_model2vec_embedding(frase_teste_amor)\n",
        "print(f\"Len Vetor da frase {len(embedding_amor)}\")\n",
        "#print(f\"Dimensão do embedding: {embedding_amor.shape[0]}\") # Correção aqui\n",
        "\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.vector\n",
        "\n",
        "frase_teste_amor = \"I love it!\"\n",
        "embedding_amor = get_mean_embedding(frase_teste_amor)\n",
        "print(f\"Len Vetor da frase {len(embedding_amor)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bCd3yX1QqBs",
        "outputId": "7ae527ea-5ebe-4626-89e2-59eb227f06eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len Vetor da frase '64\n",
            "Len Vetor da frase '300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_embedding(text):\n",
        "    emb = Model2VecEmbeddings()\n",
        "    vector_list = emb.embed_query(text=text)  # Obtém o embedding como uma lista\n",
        "    vector_np = np.array(vector_list)       # Converte a lista para um array NumPy\n",
        "    return vector_np\n",
        "\n",
        "# Testando a função com a frase \"I love it!\"\n",
        "frase_teste = \"I love it!\"\n",
        "embedding_frase = get_mean_embedding(frase_teste)\n",
        "\n",
        "print(f\"Embedding da frase '{frase_teste}':\")\n",
        "print(embedding_frase)\n",
        "print(f\"Dimensão do embedding: {len(embedding_frase[0])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "b2bRl73w-BRG",
        "outputId": "eaaf7004-5afc-41d9-a1ab-b09714b81298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding da frase 'I love it!':\n",
            "[-2.36115545e-01 -3.12402427e-01 -2.22327933e-01  1.72482193e-01\n",
            " -2.72862583e-01  5.01802981e-01 -1.04314387e-01  3.64558734e-02\n",
            "  1.62822381e-02  1.24722440e-02 -1.18248677e-02 -8.00213441e-02\n",
            "  1.17085604e-02  1.73785780e-02  9.82469916e-02 -4.03998718e-02\n",
            "  4.83643003e-02 -8.57188106e-02  6.39445335e-03  4.36404236e-02\n",
            "  3.37613001e-02 -1.24388322e-01  1.70758590e-01 -1.58254161e-01\n",
            "  1.04872413e-01  2.28477959e-02  2.67530195e-02  2.31581349e-02\n",
            "  2.92013716e-02  6.63197786e-02 -2.02581927e-01 -1.36161223e-01\n",
            "  6.31300062e-02 -1.55854970e-01  1.80398691e-02 -1.32623091e-01\n",
            "  4.46904600e-02 -7.34133506e-03 -7.21999854e-02  1.70209974e-01\n",
            " -4.70680417e-04  2.47025378e-02  5.52542582e-02 -8.42899159e-02\n",
            "  2.42101066e-02  3.58358286e-02  7.77482986e-02 -3.91708724e-02\n",
            "  6.36928827e-02  7.14799315e-02 -1.22383609e-01  2.86588389e-02\n",
            "  2.41834428e-02 -9.13440660e-02  3.47915515e-02  4.10464928e-02\n",
            " -9.48220417e-02 -5.92718879e-03  1.13304265e-01  1.96934968e-01\n",
            " -1.42617419e-01 -4.07347791e-02 -1.28535107e-01  1.08006209e-01]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'numpy.float64' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-2b7e43af0831>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Embedding da frase '{frase_teste}':\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_frase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dimensão do embedding: {len(embedding_frase[0])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "POz3DxfI-GPj",
        "outputId": "33c00bbb-af73-43e8-a740-466fe8a73bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-83d770370b39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold  # Adicionado StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder  # Importado LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import time\n",
        "from sklearn.metrics import make_scorer #importa o make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import spacy\n",
        "\n",
        "# 1. Carregar e preparar os dados\n",
        "print(\"Carregando dados...\")\n",
        "df_train = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet')\n",
        "\n",
        "if 'target' not in df_train.columns:\n",
        "    df_train['target'] = df_train['sentiment']\n",
        "\n",
        "# Codificar labels\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "y = df_train['target'].map(sentiment_mapping)\n",
        "X = df_train['comment_cleaned']\n",
        "\n",
        "# Split dos dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Dados Carregados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOioE7Bma_xx",
        "outputId": "785e69db-cb4c-4dea-e71e-725d8022dc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dados...\n",
            "Dados Carregados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model_name=\"minishlab/potion-base-2M\""
      ],
      "metadata": {
        "id": "QLBvepKBbG3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "embedding_model_instance = Model2VecEmbeddings(model_name=\"minishlab/potion-base-2M\")\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates embedding for a single text using the pre-loaded Model2VecEmbeddings instance.\n",
        "    Note: This function now uses the globally instantiated 'embedding_model_instance'.\n",
        "    The name 'get_mean_embedding' might not be strictly accurate for all models,\n",
        "    as it's now getting a single vector per text, not necessarily a mean of token vectors.\n",
        "    \"\"\"\n",
        "    # Usa a instância do Model2VecEmbeddings e chama o método para um único texto\n",
        "    # embed_query retorna uma lista de floats [float, float, ...]\n",
        "    return embedding_model_instance.embed_query(text)\n",
        "\n",
        "# # Carregar um modelo do SpaCy (escolha um que inclua embeddings)\n",
        "# nlp = spacy.load(\"en_core_web_lg\")  # ou \"en_core_web_lg\" para embeddings maiores\n",
        "\n",
        "# def get_mean_embedding(text):\n",
        "#     doc = nlp(text)\n",
        "#     return doc.vector\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "print(\"out\")\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [200],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores_acc = []\n",
        "                                    scores_recall_neg = []\n",
        "\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "                                        overall_accurary = accuracy_score(y_val_fold, y_pred_fold)\n",
        "                                        print(f\"Overall Accuracy: {overall_accurary:.4f}\")\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        scores_acc.append(overall_accurary)\n",
        "\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores_recall_neg.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "                                    mean_accuracy = np.mean(scores_acc)\n",
        "                                    print(f\"Overall Accuracy médio (validação cruzada): {mean_accuracy:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores_recall_neg)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")\n",
        "\n",
        "# New Data Validation\n",
        "\n",
        "# 1. Carregar o novo dataset\n",
        "new_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet'\n",
        "print(f\"\\n--- Avaliando no dataset: {new_data_path} ---\")\n",
        "\n",
        "try:\n",
        "    new_data = pd.read_parquet(new_data_path)\n",
        "    print(f\"Dataset '{new_data_path}' carregado com sucesso. Shape: {new_data.shape}\")\n",
        "\n",
        "    # 2. Separar textos (X_new) e rótulos (y_new)\n",
        "    # Assuma que as colunas de texto e rótulo são as mesmas do dataset de treino.\n",
        "    # Ajuste 'text_column' e 'sentiment_column' conforme necessário.\n",
        "    text_column = 'comment_cleaned'       # Nome da coluna de texto no seu parquet\n",
        "    sentiment_column = 'sentiment' # Nome da coluna de sentimento no seu parquet\n",
        "    # Codificar labels\n",
        "    sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "    # y = df_train['target'].map(sentiment_mapping)\n",
        "    # X = df_train['comment_cleaned']\n",
        "    if text_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de texto '{text_column}' não encontrada no dataset.\")\n",
        "    elif sentiment_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de sentimento '{sentiment_column}' não encontrada no dataset.\")\n",
        "    else:\n",
        "        X_new = new_data[text_column]\n",
        "        y_new = new_data[sentiment_column].map(sentiment_mapping)\n",
        "\n",
        "        # Certifique-se de que y_new está no formato numérico esperado (inteiros)\n",
        "        # Pode ser necessário aplicar o mesmo mapeamento que foi usado no treino\n",
        "        # Exemplo (se sentiment_mapping mapeia strings para ints):\n",
        "        # y_new = y_new.map({v: k for k, v in sentiment_mapping.items()})\n",
        "        # Certifique-se de que os rótulos numéricos correspondem aos usados no treino.\n",
        "        # Se y_new já vier como inteiros 0, 1, 2, essa linha acima não é necessária.\n",
        "        print(f\"Textos extraídos ({len(X_new)}). Rótulos extraídos ({len(y_new)}).\")\n",
        "        print(\"Primeiros 5 rótulos do novo dataset:\", y_new.head().tolist())\n",
        "\n",
        "\n",
        "        # 3. Gerar embeddings para o novo dataset\n",
        "        print(\"Gerando embeddings para o novo dataset...\")\n",
        "\n",
        "        # É crucial usar a mesma instância do embedder já carregado\n",
        "        # A sua classe Model2VecEmbeddings tem o método embed_documents para listas,\n",
        "        # que é mais eficiente do que chamar embed_query em loop.\n",
        "        if 'embedding_model_instance' in globals() and embedding_model_instance is not None:\n",
        "             X_new_vec_list = embedding_model_instance.embed_documents(X_new.tolist())\n",
        "             X_new_vec = np.array(X_new_vec_list)\n",
        "             print(f\"Embeddings para o novo dataset gerados: {X_new_vec.shape}\")\n",
        "\n",
        "             # 4. Avaliar o best_model no novo conjunto de embeddings\n",
        "             print(\"\\nAvaliando o melhor modelo no novo dataset...\")\n",
        "\n",
        "             if 'best_model' in globals() and best_model is not None:\n",
        "                 # Fazer as predições usando o modelo treinado\n",
        "                 y_pred_new_proba = best_model.predict(X_new_vec)\n",
        "                 y_pred_new = np.argmax(y_pred_new_proba, axis=-1) # Converter predições para rótulos\n",
        "\n",
        "                 # 5. Exibir métricas de avaliação\n",
        "                 print(\"\\nRelatório de Avaliação no Novo Dataset:\")\n",
        "                 # Use y_new e y_pred_new\n",
        "                 test_accuracy_new = accuracy_score(y_new, y_pred_new)\n",
        "                 test_f1_new = f1_score(y_new, y_pred_new, average='weighted')\n",
        "                 recall_negativo_new = recall_score(y_new, y_pred_new, average=None, labels=[0])[0] # Recall para a classe 0 (Negativo)\n",
        "\n",
        "                 print(f\"Acurácia no novo dataset: {test_accuracy_new:.4f}\")\n",
        "                 print(f\"F1-Score no novo dataset: {test_f1_new:.4f}\")\n",
        "                 print(f\"Recall para classe 'Negative' (0) no novo dataset: {recall_negativo_new:.4f}\")\n",
        "\n",
        "                 print(\"\\nClassification Report no Novo Dataset:\")\n",
        "                 # Assegure-se de que sentiment_mapping está definido e correto para target_names\n",
        "                 if 'sentiment_mapping' in globals():\n",
        "                     target_names_list = [str(sentiment_mapping[i]) for i in sorted(sentiment_mapping.keys())]\n",
        "                     print(classification_report(y_new, y_pred_new, target_names=target_names_list))\n",
        "                 else:\n",
        "                      print(classification_report(y_new, y_pred_new)) # Sem nomes se mapping não disponível\n",
        "\n",
        "                 # Opcional: Análise de Falsos Negativos no NOVO dataset\n",
        "                 print(\"\\nAnálise de Falsos Negativos no Novo Dataset:\")\n",
        "                 # Encontrar os índices onde o rótulo real é 0 (Negative) mas a predição não é 0\n",
        "                 false_negatives_indices_new = new_data.index[(y_new == 0) & (y_pred_new != 0)]\n",
        "\n",
        "                 if len(false_negatives_indices_new) > 0:\n",
        "                     print(f\"Encontrados {len(false_negatives_indices_new)} falsos negativos no novo dataset.\")\n",
        "                     print(\"Exemplos de falsos negativos (primeiros 5):\")\n",
        "                     # Use .loc com os índices do dataframe original para pegar os textos e rótulos\n",
        "                     for i, idx in enumerate(false_negatives_indices_new[:5]):\n",
        "                         original_text = new_data.loc[idx, text_column]\n",
        "                         true_label = y_new.loc[idx]\n",
        "                         predicted_label = y_pred_new[new_data.index.get_loc(idx)] # Encontra a predição pelo índice original\n",
        "                         # Mapear rótulos numéricos para nomes se sentiment_mapping estiver disponível\n",
        "                         true_label_name = sentiment_mapping.get(true_label, str(true_label)) if 'sentiment_mapping' in globals() else str(true_label)\n",
        "                         predicted_label_name = sentiment_mapping.get(predicted_label, str(predicted_label)) if 'sentiment_mapping' in globals() else str(predicted_label)\n",
        "\n",
        "                         print(f\"  {i+1}. Texto: '{original_text}'\")\n",
        "                         print(f\"     Real: {true_label_name} ({true_label}), Predito: {predicted_label_name} ({predicted_label})\")\n",
        "                 else:\n",
        "                     print(\"Nenhum falso negativo encontrado no novo dataset.\")\n",
        "\n",
        "             else:\n",
        "                 print(\"Erro: A variável 'best_model' não foi encontrada ou é None. Certifique-se de que o treinamento e a seleção do modelo foram executados antes da avaliação.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: Arquivo '{new_data_path}' não encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante o processamento do novo dataset: {e}\")\n",
        "\n",
        "print(\"\\n--- Fim da avaliação no novo dataset ---\")"
      ],
      "metadata": {
        "id": "cNTA2I5PwiE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2528838e-f289-4f9f-91a1-5f0abb8d3c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vetorizando textos...\n",
            "out\n",
            "\n",
            "Configurando Grid Search...\n",
            "\n",
            "Iniciando Grid Search...\n",
            "\n",
            "Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=200, activation=relu, regularizer=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Overall Accuracy: 0.6327\n",
            "Recall (Negativo): 0.6812\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Overall Accuracy: 0.9436\n",
            "Recall (Negativo): 0.9429\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Overall Accuracy: 1.0000\n",
            "Recall (Negativo): 1.0000\n",
            "Overall Accuracy médio (validação cruzada): 0.8587\n",
            "Recall (Negativo) médio (validação cruzada): 0.8747\n",
            "------------------------------\n",
            "\n",
            "Tempo total de execução: 1.11 minutos\n",
            "\n",
            "Melhores parâmetros encontrados:\n",
            "units1: 128\n",
            "units2: 32\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.001\n",
            "optimizer: adam\n",
            "batch_size: 32\n",
            "epochs: 200\n",
            "activation: relu\n",
            "regularizer: None\n",
            "\n",
            "Melhor score de validação: 0.8747\n",
            "\n",
            "Avaliando no conjunto de teste...\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "\n",
            "Acurácia no teste: 0.7007\n",
            "F1-Score no teste: 0.7010\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.65      0.70        52\n",
            "     Neutral       0.63      0.74      0.68        35\n",
            "    Positive       0.70      0.72      0.71        60\n",
            "\n",
            "    accuracy                           0.70       147\n",
            "   macro avg       0.70      0.70      0.70       147\n",
            "weighted avg       0.71      0.70      0.70       147\n",
            "\n",
            "\n",
            "Métricas para classe Negative:\n",
            "Acurácia para Negative: 0.6538\n",
            "F1-Score para Negative: 0.7907\n",
            "\n",
            "Exemplos de falsos negativos:\n",
            "1. i had to ask her three times before she finally came back with the dish ive requested .;\n",
            "2. and i hate to say this but i doubt i ' ll ever go back .;\n",
            "3. i know , you were too busy showing off your vintage tee shirt and looking bored , but my agenda is i ' m here to eat and enjoy the company of friends , seeking a pleasant experience .;\n",
            "4. the service varys from day to day - sometimes they ' re very nice , and sometimes not .;\n",
            "5. first went here to enjoy their garden terrace .;\n",
            "\n",
            "--- Avaliando no dataset: /content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet ---\n",
            "Dataset '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet' carregado com sucesso. Shape: (199, 13)\n",
            "Textos extraídos (199). Rótulos extraídos (199).\n",
            "Primeiros 5 rótulos do novo dataset: [2, 0, 1, 2, 2]\n",
            "Gerando embeddings para o novo dataset...\n",
            "Embeddings para o novo dataset gerados: (199, 64)\n",
            "\n",
            "Avaliando o melhor modelo no novo dataset...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\n",
            "Relatório de Avaliação no Novo Dataset:\n",
            "Acurácia no novo dataset: 0.6834\n",
            "F1-Score no novo dataset: 0.6850\n",
            "Recall para classe 'Negative' (0) no novo dataset: 0.6600\n",
            "\n",
            "Classification Report no Novo Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.61        50\n",
            "           1       0.17      0.16      0.17        25\n",
            "           2       0.85      0.80      0.82       124\n",
            "\n",
            "    accuracy                           0.68       199\n",
            "   macro avg       0.53      0.54      0.53       199\n",
            "weighted avg       0.69      0.68      0.69       199\n",
            "\n",
            "\n",
            "Análise de Falsos Negativos no Novo Dataset:\n",
            "Encontrados 17 falsos negativos no novo dataset.\n",
            "Exemplos de falsos negativos (primeiros 5):\n",
            "  1. Texto: 'the bartender on my most recent visit was so incredibly rude that i will never go back .'\n",
            "     Real: 0 (0), Predito: 2 (2)\n",
            "  2. Texto: 'the strong scents coming from the left and right of me negatively affected my taste buds .'\n",
            "     Real: 0 (0), Predito: 2 (2)\n",
            "  3. Texto: 'the environment is romantic , but the food is horrible , the service is pathetic , and gabriella lies about everything she could .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  4. Texto: 'and at the conclusion of this culinary nightmare the check was given to our table by knocking over a glass of water .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  5. Texto: 'all the money went into the interior decoration , none of it went to the chefs .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "\n",
            "--- Fim da avaliação no novo dataset ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model_name=\"minishlab/potion-base-4M\""
      ],
      "metadata": {
        "id": "VFTSVNPna5Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "embedding_model_instance = Model2VecEmbeddings(model_name=\"minishlab/potion-base-4M\")\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates embedding for a single text using the pre-loaded Model2VecEmbeddings instance.\n",
        "    Note: This function now uses the globally instantiated 'embedding_model_instance'.\n",
        "    The name 'get_mean_embedding' might not be strictly accurate for all models,\n",
        "    as it's now getting a single vector per text, not necessarily a mean of token vectors.\n",
        "    \"\"\"\n",
        "    # Usa a instância do Model2VecEmbeddings e chama o método para um único texto\n",
        "    # embed_query retorna uma lista de floats [float, float, ...]\n",
        "    return embedding_model_instance.embed_query(text)\n",
        "\n",
        "# # Carregar um modelo do SpaCy (escolha um que inclua embeddings)\n",
        "# nlp = spacy.load(\"en_core_web_lg\")  # ou \"en_core_web_lg\" para embeddings maiores\n",
        "\n",
        "# def get_mean_embedding(text):\n",
        "#     doc = nlp(text)\n",
        "#     return doc.vector\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "print(\"out\")\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [200],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores_acc = []\n",
        "                                    scores_recall_neg = []\n",
        "\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "                                        overall_accurary = accuracy_score(y_val_fold, y_pred_fold)\n",
        "                                        print(f\"Overall Accuracy: {overall_accurary:.4f}\")\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        scores_acc.append(overall_accurary)\n",
        "\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores_recall_neg.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "                                    mean_accuracy = np.mean(scores_acc)\n",
        "                                    print(f\"Overall Accuracy médio (validação cruzada): {mean_accuracy:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores_recall_neg)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")\n",
        "\n",
        "# New Data Validation\n",
        "\n",
        "# 1. Carregar o novo dataset\n",
        "new_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet'\n",
        "print(f\"\\n--- Avaliando no dataset: {new_data_path} ---\")\n",
        "\n",
        "try:\n",
        "    new_data = pd.read_parquet(new_data_path)\n",
        "    print(f\"Dataset '{new_data_path}' carregado com sucesso. Shape: {new_data.shape}\")\n",
        "\n",
        "    # 2. Separar textos (X_new) e rótulos (y_new)\n",
        "    # Assuma que as colunas de texto e rótulo são as mesmas do dataset de treino.\n",
        "    # Ajuste 'text_column' e 'sentiment_column' conforme necessário.\n",
        "    text_column = 'comment_cleaned'       # Nome da coluna de texto no seu parquet\n",
        "    sentiment_column = 'sentiment' # Nome da coluna de sentimento no seu parquet\n",
        "    # Codificar labels\n",
        "    sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "    # y = df_train['target'].map(sentiment_mapping)\n",
        "    # X = df_train['comment_cleaned']\n",
        "    if text_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de texto '{text_column}' não encontrada no dataset.\")\n",
        "    elif sentiment_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de sentimento '{sentiment_column}' não encontrada no dataset.\")\n",
        "    else:\n",
        "        X_new = new_data[text_column]\n",
        "        y_new = new_data[sentiment_column].map(sentiment_mapping)\n",
        "\n",
        "        # Certifique-se de que y_new está no formato numérico esperado (inteiros)\n",
        "        # Pode ser necessário aplicar o mesmo mapeamento que foi usado no treino\n",
        "        # Exemplo (se sentiment_mapping mapeia strings para ints):\n",
        "        # y_new = y_new.map({v: k for k, v in sentiment_mapping.items()})\n",
        "        # Certifique-se de que os rótulos numéricos correspondem aos usados no treino.\n",
        "        # Se y_new já vier como inteiros 0, 1, 2, essa linha acima não é necessária.\n",
        "        print(f\"Textos extraídos ({len(X_new)}). Rótulos extraídos ({len(y_new)}).\")\n",
        "        print(\"Primeiros 5 rótulos do novo dataset:\", y_new.head().tolist())\n",
        "\n",
        "\n",
        "        # 3. Gerar embeddings para o novo dataset\n",
        "        print(\"Gerando embeddings para o novo dataset...\")\n",
        "\n",
        "        # É crucial usar a mesma instância do embedder já carregado\n",
        "        # A sua classe Model2VecEmbeddings tem o método embed_documents para listas,\n",
        "        # que é mais eficiente do que chamar embed_query em loop.\n",
        "        if 'embedding_model_instance' in globals() and embedding_model_instance is not None:\n",
        "             X_new_vec_list = embedding_model_instance.embed_documents(X_new.tolist())\n",
        "             X_new_vec = np.array(X_new_vec_list)\n",
        "             print(f\"Embeddings para o novo dataset gerados: {X_new_vec.shape}\")\n",
        "\n",
        "             # 4. Avaliar o best_model no novo conjunto de embeddings\n",
        "             print(\"\\nAvaliando o melhor modelo no novo dataset...\")\n",
        "\n",
        "             if 'best_model' in globals() and best_model is not None:\n",
        "                 # Fazer as predições usando o modelo treinado\n",
        "                 y_pred_new_proba = best_model.predict(X_new_vec)\n",
        "                 y_pred_new = np.argmax(y_pred_new_proba, axis=-1) # Converter predições para rótulos\n",
        "\n",
        "                 # 5. Exibir métricas de avaliação\n",
        "                 print(\"\\nRelatório de Avaliação no Novo Dataset:\")\n",
        "                 # Use y_new e y_pred_new\n",
        "                 test_accuracy_new = accuracy_score(y_new, y_pred_new)\n",
        "                 test_f1_new = f1_score(y_new, y_pred_new, average='weighted')\n",
        "                 recall_negativo_new = recall_score(y_new, y_pred_new, average=None, labels=[0])[0] # Recall para a classe 0 (Negativo)\n",
        "\n",
        "                 print(f\"Acurácia no novo dataset: {test_accuracy_new:.4f}\")\n",
        "                 print(f\"F1-Score no novo dataset: {test_f1_new:.4f}\")\n",
        "                 print(f\"Recall para classe 'Negative' (0) no novo dataset: {recall_negativo_new:.4f}\")\n",
        "\n",
        "                 print(\"\\nClassification Report no Novo Dataset:\")\n",
        "                 # Assegure-se de que sentiment_mapping está definido e correto para target_names\n",
        "                 if 'sentiment_mapping' in globals():\n",
        "                     target_names_list = [str(sentiment_mapping[i]) for i in sorted(sentiment_mapping.keys())]\n",
        "                     print(classification_report(y_new, y_pred_new, target_names=target_names_list))\n",
        "                 else:\n",
        "                      print(classification_report(y_new, y_pred_new)) # Sem nomes se mapping não disponível\n",
        "\n",
        "                 # Opcional: Análise de Falsos Negativos no NOVO dataset\n",
        "                 print(\"\\nAnálise de Falsos Negativos no Novo Dataset:\")\n",
        "                 # Encontrar os índices onde o rótulo real é 0 (Negative) mas a predição não é 0\n",
        "                 false_negatives_indices_new = new_data.index[(y_new == 0) & (y_pred_new != 0)]\n",
        "\n",
        "                 if len(false_negatives_indices_new) > 0:\n",
        "                     print(f\"Encontrados {len(false_negatives_indices_new)} falsos negativos no novo dataset.\")\n",
        "                     print(\"Exemplos de falsos negativos (primeiros 5):\")\n",
        "                     # Use .loc com os índices do dataframe original para pegar os textos e rótulos\n",
        "                     for i, idx in enumerate(false_negatives_indices_new[:5]):\n",
        "                         original_text = new_data.loc[idx, text_column]\n",
        "                         true_label = y_new.loc[idx]\n",
        "                         predicted_label = y_pred_new[new_data.index.get_loc(idx)] # Encontra a predição pelo índice original\n",
        "                         # Mapear rótulos numéricos para nomes se sentiment_mapping estiver disponível\n",
        "                         true_label_name = sentiment_mapping.get(true_label, str(true_label)) if 'sentiment_mapping' in globals() else str(true_label)\n",
        "                         predicted_label_name = sentiment_mapping.get(predicted_label, str(predicted_label)) if 'sentiment_mapping' in globals() else str(predicted_label)\n",
        "\n",
        "                         print(f\"  {i+1}. Texto: '{original_text}'\")\n",
        "                         print(f\"     Real: {true_label_name} ({true_label}), Predito: {predicted_label_name} ({predicted_label})\")\n",
        "                 else:\n",
        "                     print(\"Nenhum falso negativo encontrado no novo dataset.\")\n",
        "\n",
        "             else:\n",
        "                 print(\"Erro: A variável 'best_model' não foi encontrada ou é None. Certifique-se de que o treinamento e a seleção do modelo foram executados antes da avaliação.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: Arquivo '{new_data_path}' não encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante o processamento do novo dataset: {e}\")\n",
        "\n",
        "print(\"\\n--- Fim da avaliação no novo dataset ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKCupacYa1ns",
        "outputId": "48b38a55-6497-4f75-9113-bc82e7b6614e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vetorizando textos...\n",
            "out\n",
            "\n",
            "Configurando Grid Search...\n",
            "\n",
            "Iniciando Grid Search...\n",
            "\n",
            "Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=200, activation=relu, regularizer=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Overall Accuracy: 0.6480\n",
            "Recall (Negativo): 0.6232\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Overall Accuracy: 0.9692\n",
            "Recall (Negativo): 0.9857\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Overall Accuracy: 1.0000\n",
            "Recall (Negativo): 1.0000\n",
            "Overall Accuracy médio (validação cruzada): 0.8724\n",
            "Recall (Negativo) médio (validação cruzada): 0.8696\n",
            "------------------------------\n",
            "\n",
            "Tempo total de execução: 1.14 minutos\n",
            "\n",
            "Melhores parâmetros encontrados:\n",
            "units1: 128\n",
            "units2: 32\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.001\n",
            "optimizer: adam\n",
            "batch_size: 32\n",
            "epochs: 200\n",
            "activation: relu\n",
            "regularizer: None\n",
            "\n",
            "Melhor score de validação: 0.8696\n",
            "\n",
            "Avaliando no conjunto de teste...\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\n",
            "Acurácia no teste: 0.7551\n",
            "F1-Score no teste: 0.7551\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.71      0.73        52\n",
            "     Neutral       0.68      0.86      0.76        35\n",
            "    Positive       0.81      0.73      0.77        60\n",
            "\n",
            "    accuracy                           0.76       147\n",
            "   macro avg       0.75      0.77      0.75       147\n",
            "weighted avg       0.76      0.76      0.76       147\n",
            "\n",
            "\n",
            "Métricas para classe Negative:\n",
            "Acurácia para Negative: 0.7115\n",
            "F1-Score para Negative: 0.8315\n",
            "\n",
            "Exemplos de falsos negativos:\n",
            "1. ive asked a cart attendant for a lotus leaf wrapped rice and she replied back rice and just walked away .;\n",
            "2. i know , you were too busy showing off your vintage tee shirt and looking bored , but my agenda is i ' m here to eat and enjoy the company of friends , seeking a pleasant experience .;\n",
            "3. i went there for lunch and it wasnot as good as i expected from the reviews i read .;\n",
            "4. first it took us a long time to find the place .;\n",
            "5. but when we looked at the menu , there weren't a lot of choices , most of them were dumplings in the appetizer section .;\n",
            "\n",
            "--- Avaliando no dataset: /content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet ---\n",
            "Dataset '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet' carregado com sucesso. Shape: (199, 13)\n",
            "Textos extraídos (199). Rótulos extraídos (199).\n",
            "Primeiros 5 rótulos do novo dataset: [2, 0, 1, 2, 2]\n",
            "Gerando embeddings para o novo dataset...\n",
            "Embeddings para o novo dataset gerados: (199, 128)\n",
            "\n",
            "Avaliando o melhor modelo no novo dataset...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
            "\n",
            "Relatório de Avaliação no Novo Dataset:\n",
            "Acurácia no novo dataset: 0.6432\n",
            "F1-Score no novo dataset: 0.6550\n",
            "Recall para classe 'Negative' (0) no novo dataset: 0.6600\n",
            "\n",
            "Classification Report no Novo Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.66      0.57        50\n",
            "           1       0.11      0.12      0.12        25\n",
            "           2       0.86      0.74      0.80       124\n",
            "\n",
            "    accuracy                           0.64       199\n",
            "   macro avg       0.49      0.51      0.50       199\n",
            "weighted avg       0.68      0.64      0.66       199\n",
            "\n",
            "\n",
            "Análise de Falsos Negativos no Novo Dataset:\n",
            "Encontrados 17 falsos negativos no novo dataset.\n",
            "Exemplos de falsos negativos (primeiros 5):\n",
            "  1. Texto: 'if you 're not familiar with the cuisine , you may encounter problems ordering here and be sure to bring plenty of cash with you as they don't accept credit cards .'\n",
            "     Real: 0 (0), Predito: 2 (2)\n",
            "  2. Texto: 'all the money went into the interior decoration , none of it went to the chefs .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  3. Texto: 'for those prices and the dressed up atmosphere you expect more and should get more .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  4. Texto: 'don't go alone -- -even two people isn't enough for the whole experience , with pickles and a selection of meats and seafoods .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  5. Texto: 'the pizza was delivered cold and the cheese wasn't even fully melted !'\n",
            "     Real: 0 (0), Predito: 2 (2)\n",
            "\n",
            "--- Fim da avaliação no novo dataset ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8M"
      ],
      "metadata": {
        "id": "8EStLj0ji4Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "embedding_model_instance = Model2VecEmbeddings(model_name=\"minishlab/potion-base-8M\")\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates embedding for a single text using the pre-loaded Model2VecEmbeddings instance.\n",
        "    Note: This function now uses the globally instantiated 'embedding_model_instance'.\n",
        "    The name 'get_mean_embedding' might not be strictly accurate for all models,\n",
        "    as it's now getting a single vector per text, not necessarily a mean of token vectors.\n",
        "    \"\"\"\n",
        "    # Usa a instância do Model2VecEmbeddings e chama o método para um único texto\n",
        "    # embed_query retorna uma lista de floats [float, float, ...]\n",
        "    return embedding_model_instance.embed_query(text)\n",
        "\n",
        "# # Carregar um modelo do SpaCy (escolha um que inclua embeddings)\n",
        "# nlp = spacy.load(\"en_core_web_lg\")  # ou \"en_core_web_lg\" para embeddings maiores\n",
        "\n",
        "# def get_mean_embedding(text):\n",
        "#     doc = nlp(text)\n",
        "#     return doc.vector\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "print(\"out\")\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [200],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores_acc = []\n",
        "                                    scores_recall_neg = []\n",
        "\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "                                        overall_accurary = accuracy_score(y_val_fold, y_pred_fold)\n",
        "                                        print(f\"Overall Accuracy: {overall_accurary:.4f}\")\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        scores_acc.append(overall_accurary)\n",
        "\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores_recall_neg.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "                                    mean_accuracy = np.mean(scores_acc)\n",
        "                                    print(f\"Overall Accuracy médio (validação cruzada): {mean_accuracy:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores_recall_neg)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")\n",
        "\n",
        "# New Data Validation\n",
        "\n",
        "# 1. Carregar o novo dataset\n",
        "new_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet'\n",
        "print(f\"\\n--- Avaliando no dataset: {new_data_path} ---\")\n",
        "\n",
        "try:\n",
        "    new_data = pd.read_parquet(new_data_path)\n",
        "    print(f\"Dataset '{new_data_path}' carregado com sucesso. Shape: {new_data.shape}\")\n",
        "\n",
        "    # 2. Separar textos (X_new) e rótulos (y_new)\n",
        "    # Assuma que as colunas de texto e rótulo são as mesmas do dataset de treino.\n",
        "    # Ajuste 'text_column' e 'sentiment_column' conforme necessário.\n",
        "    text_column = 'comment_cleaned'       # Nome da coluna de texto no seu parquet\n",
        "    sentiment_column = 'sentiment' # Nome da coluna de sentimento no seu parquet\n",
        "    # Codificar labels\n",
        "    sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "    # y = df_train['target'].map(sentiment_mapping)\n",
        "    # X = df_train['comment_cleaned']\n",
        "    if text_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de texto '{text_column}' não encontrada no dataset.\")\n",
        "    elif sentiment_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de sentimento '{sentiment_column}' não encontrada no dataset.\")\n",
        "    else:\n",
        "        X_new = new_data[text_column]\n",
        "        y_new = new_data[sentiment_column].map(sentiment_mapping)\n",
        "\n",
        "        # Certifique-se de que y_new está no formato numérico esperado (inteiros)\n",
        "        # Pode ser necessário aplicar o mesmo mapeamento que foi usado no treino\n",
        "        # Exemplo (se sentiment_mapping mapeia strings para ints):\n",
        "        # y_new = y_new.map({v: k for k, v in sentiment_mapping.items()})\n",
        "        # Certifique-se de que os rótulos numéricos correspondem aos usados no treino.\n",
        "        # Se y_new já vier como inteiros 0, 1, 2, essa linha acima não é necessária.\n",
        "        print(f\"Textos extraídos ({len(X_new)}). Rótulos extraídos ({len(y_new)}).\")\n",
        "        print(\"Primeiros 5 rótulos do novo dataset:\", y_new.head().tolist())\n",
        "\n",
        "\n",
        "        # 3. Gerar embeddings para o novo dataset\n",
        "        print(\"Gerando embeddings para o novo dataset...\")\n",
        "\n",
        "        # É crucial usar a mesma instância do embedder já carregado\n",
        "        # A sua classe Model2VecEmbeddings tem o método embed_documents para listas,\n",
        "        # que é mais eficiente do que chamar embed_query em loop.\n",
        "        if 'embedding_model_instance' in globals() and embedding_model_instance is not None:\n",
        "             X_new_vec_list = embedding_model_instance.embed_documents(X_new.tolist())\n",
        "             X_new_vec = np.array(X_new_vec_list)\n",
        "             print(f\"Embeddings para o novo dataset gerados: {X_new_vec.shape}\")\n",
        "\n",
        "             # 4. Avaliar o best_model no novo conjunto de embeddings\n",
        "             print(\"\\nAvaliando o melhor modelo no novo dataset...\")\n",
        "\n",
        "             if 'best_model' in globals() and best_model is not None:\n",
        "                 # Fazer as predições usando o modelo treinado\n",
        "                 y_pred_new_proba = best_model.predict(X_new_vec)\n",
        "                 y_pred_new = np.argmax(y_pred_new_proba, axis=-1) # Converter predições para rótulos\n",
        "\n",
        "                 # 5. Exibir métricas de avaliação\n",
        "                 print(\"\\nRelatório de Avaliação no Novo Dataset:\")\n",
        "                 # Use y_new e y_pred_new\n",
        "                 test_accuracy_new = accuracy_score(y_new, y_pred_new)\n",
        "                 test_f1_new = f1_score(y_new, y_pred_new, average='weighted')\n",
        "                 recall_negativo_new = recall_score(y_new, y_pred_new, average=None, labels=[0])[0] # Recall para a classe 0 (Negativo)\n",
        "\n",
        "                 print(f\"Acurácia no novo dataset: {test_accuracy_new:.4f}\")\n",
        "                 print(f\"F1-Score no novo dataset: {test_f1_new:.4f}\")\n",
        "                 print(f\"Recall para classe 'Negative' (0) no novo dataset: {recall_negativo_new:.4f}\")\n",
        "\n",
        "                 print(\"\\nClassification Report no Novo Dataset:\")\n",
        "                 # Assegure-se de que sentiment_mapping está definido e correto para target_names\n",
        "                 if 'sentiment_mapping' in globals():\n",
        "                     target_names_list = [str(sentiment_mapping[i]) for i in sorted(sentiment_mapping.keys())]\n",
        "                     print(classification_report(y_new, y_pred_new, target_names=target_names_list))\n",
        "                 else:\n",
        "                      print(classification_report(y_new, y_pred_new)) # Sem nomes se mapping não disponível\n",
        "\n",
        "                 # Opcional: Análise de Falsos Negativos no NOVO dataset\n",
        "                 print(\"\\nAnálise de Falsos Negativos no Novo Dataset:\")\n",
        "                 # Encontrar os índices onde o rótulo real é 0 (Negative) mas a predição não é 0\n",
        "                 false_negatives_indices_new = new_data.index[(y_new == 0) & (y_pred_new != 0)]\n",
        "\n",
        "                 if len(false_negatives_indices_new) > 0:\n",
        "                     print(f\"Encontrados {len(false_negatives_indices_new)} falsos negativos no novo dataset.\")\n",
        "                     print(\"Exemplos de falsos negativos (primeiros 5):\")\n",
        "                     # Use .loc com os índices do dataframe original para pegar os textos e rótulos\n",
        "                     for i, idx in enumerate(false_negatives_indices_new[:5]):\n",
        "                         original_text = new_data.loc[idx, text_column]\n",
        "                         true_label = y_new.loc[idx]\n",
        "                         predicted_label = y_pred_new[new_data.index.get_loc(idx)] # Encontra a predição pelo índice original\n",
        "                         # Mapear rótulos numéricos para nomes se sentiment_mapping estiver disponível\n",
        "                         true_label_name = sentiment_mapping.get(true_label, str(true_label)) if 'sentiment_mapping' in globals() else str(true_label)\n",
        "                         predicted_label_name = sentiment_mapping.get(predicted_label, str(predicted_label)) if 'sentiment_mapping' in globals() else str(predicted_label)\n",
        "\n",
        "                         print(f\"  {i+1}. Texto: '{original_text}'\")\n",
        "                         print(f\"     Real: {true_label_name} ({true_label}), Predito: {predicted_label_name} ({predicted_label})\")\n",
        "                 else:\n",
        "                     print(\"Nenhum falso negativo encontrado no novo dataset.\")\n",
        "\n",
        "             else:\n",
        "                 print(\"Erro: A variável 'best_model' não foi encontrada ou é None. Certifique-se de que o treinamento e a seleção do modelo foram executados antes da avaliação.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: Arquivo '{new_data_path}' não encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante o processamento do novo dataset: {e}\")\n",
        "\n",
        "print(\"\\n--- Fim da avaliação no novo dataset ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoiG666gi6aR",
        "outputId": "64707b04-9275-4aaa-cd60-d9ea6b493d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vetorizando textos...\n",
            "out\n",
            "\n",
            "Configurando Grid Search...\n",
            "\n",
            "Iniciando Grid Search...\n",
            "\n",
            "Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=200, activation=relu, regularizer=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Overall Accuracy: 0.6582\n",
            "Recall (Negativo): 0.6522\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Overall Accuracy: 0.9641\n",
            "Recall (Negativo): 0.9714\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Overall Accuracy: 1.0000\n",
            "Recall (Negativo): 1.0000\n",
            "Overall Accuracy médio (validação cruzada): 0.8741\n",
            "Recall (Negativo) médio (validação cruzada): 0.8745\n",
            "------------------------------\n",
            "\n",
            "Tempo total de execução: 1.11 minutos\n",
            "\n",
            "Melhores parâmetros encontrados:\n",
            "units1: 128\n",
            "units2: 32\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.001\n",
            "optimizer: adam\n",
            "batch_size: 32\n",
            "epochs: 200\n",
            "activation: relu\n",
            "regularizer: None\n",
            "\n",
            "Melhor score de validação: 0.8745\n",
            "\n",
            "Avaliando no conjunto de teste...\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\n",
            "Acurácia no teste: 0.7279\n",
            "F1-Score no teste: 0.7277\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.76      0.67      0.71        52\n",
            "     Neutral       0.67      0.80      0.73        35\n",
            "    Positive       0.75      0.73      0.74        60\n",
            "\n",
            "    accuracy                           0.73       147\n",
            "   macro avg       0.72      0.74      0.73       147\n",
            "weighted avg       0.73      0.73      0.73       147\n",
            "\n",
            "\n",
            "Métricas para classe Negative:\n",
            "Acurácia para Negative: 0.6731\n",
            "F1-Score para Negative: 0.8046\n",
            "\n",
            "Exemplos de falsos negativos:\n",
            "1. ive asked a cart attendant for a lotus leaf wrapped rice and she replied back rice and just walked away .;\n",
            "2. i had to ask her three times before she finally came back with the dish ive requested .;\n",
            "3. i know , you were too busy showing off your vintage tee shirt and looking bored , but my agenda is i ' m here to eat and enjoy the company of friends , seeking a pleasant experience .;\n",
            "4. i went there for lunch and it wasnot as good as i expected from the reviews i read .;\n",
            "5. but when we looked at the menu , there weren't a lot of choices , most of them were dumplings in the appetizer section .;\n",
            "\n",
            "--- Avaliando no dataset: /content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet ---\n",
            "Dataset '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet' carregado com sucesso. Shape: (199, 13)\n",
            "Textos extraídos (199). Rótulos extraídos (199).\n",
            "Primeiros 5 rótulos do novo dataset: [2, 0, 1, 2, 2]\n",
            "Gerando embeddings para o novo dataset...\n",
            "Embeddings para o novo dataset gerados: (199, 256)\n",
            "\n",
            "Avaliando o melhor modelo no novo dataset...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\n",
            "Relatório de Avaliação no Novo Dataset:\n",
            "Acurácia no novo dataset: 0.6583\n",
            "F1-Score no novo dataset: 0.6741\n",
            "Recall para classe 'Negative' (0) no novo dataset: 0.7800\n",
            "\n",
            "Classification Report no Novo Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.78      0.63        50\n",
            "           1       0.26      0.32      0.29        25\n",
            "           2       0.89      0.68      0.77       124\n",
            "\n",
            "    accuracy                           0.66       199\n",
            "   macro avg       0.56      0.59      0.56       199\n",
            "weighted avg       0.72      0.66      0.67       199\n",
            "\n",
            "\n",
            "Análise de Falsos Negativos no Novo Dataset:\n",
            "Encontrados 11 falsos negativos no novo dataset.\n",
            "Exemplos de falsos negativos (primeiros 5):\n",
            "  1. Texto: 'the strong scents coming from the left and right of me negatively affected my taste buds .'\n",
            "     Real: 0 (0), Predito: 2 (2)\n",
            "  2. Texto: 'if you 're not familiar with the cuisine , you may encounter problems ordering here and be sure to bring plenty of cash with you as they don't accept credit cards .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  3. Texto: 'besides , when you have bad service , that 's less money you have to tip .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  4. Texto: 'for those prices and the dressed up atmosphere you expect more and should get more .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  5. Texto: 'don't go alone -- -even two people isn't enough for the whole experience , with pickles and a selection of meats and seafoods .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "\n",
            "--- Fim da avaliação no novo dataset ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 32M"
      ],
      "metadata": {
        "id": "OIRWPZ6HjTuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "embedding_model_instance = Model2VecEmbeddings(model_name=\"minishlab/potion-base-32M\")\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates embedding for a single text using the pre-loaded Model2VecEmbeddings instance.\n",
        "    Note: This function now uses the globally instantiated 'embedding_model_instance'.\n",
        "    The name 'get_mean_embedding' might not be strictly accurate for all models,\n",
        "    as it's now getting a single vector per text, not necessarily a mean of token vectors.\n",
        "    \"\"\"\n",
        "    # Usa a instância do Model2VecEmbeddings e chama o método para um único texto\n",
        "    # embed_query retorna uma lista de floats [float, float, ...]\n",
        "    return embedding_model_instance.embed_query(text)\n",
        "\n",
        "# # Carregar um modelo do SpaCy (escolha um que inclua embeddings)\n",
        "# nlp = spacy.load(\"en_core_web_lg\")  # ou \"en_core_web_lg\" para embeddings maiores\n",
        "\n",
        "# def get_mean_embedding(text):\n",
        "#     doc = nlp(text)\n",
        "#     return doc.vector\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "print(\"out\")\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [200],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores_acc = []\n",
        "                                    scores_recall_neg = []\n",
        "\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "                                        overall_accurary = accuracy_score(y_val_fold, y_pred_fold)\n",
        "                                        print(f\"Overall Accuracy: {overall_accurary:.4f}\")\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        scores_acc.append(overall_accurary)\n",
        "\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores_recall_neg.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "                                    mean_accuracy = np.mean(scores_acc)\n",
        "                                    print(f\"Overall Accuracy médio (validação cruzada): {mean_accuracy:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores_recall_neg)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")\n",
        "\n",
        "# New Data Validation\n",
        "\n",
        "# 1. Carregar o novo dataset\n",
        "new_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet'\n",
        "print(f\"\\n--- Avaliando no dataset: {new_data_path} ---\")\n",
        "\n",
        "try:\n",
        "    new_data = pd.read_parquet(new_data_path)\n",
        "    print(f\"Dataset '{new_data_path}' carregado com sucesso. Shape: {new_data.shape}\")\n",
        "\n",
        "    # 2. Separar textos (X_new) e rótulos (y_new)\n",
        "    # Assuma que as colunas de texto e rótulo são as mesmas do dataset de treino.\n",
        "    # Ajuste 'text_column' e 'sentiment_column' conforme necessário.\n",
        "    text_column = 'comment_cleaned'       # Nome da coluna de texto no seu parquet\n",
        "    sentiment_column = 'sentiment' # Nome da coluna de sentimento no seu parquet\n",
        "    # Codificar labels\n",
        "    sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "    # y = df_train['target'].map(sentiment_mapping)\n",
        "    # X = df_train['comment_cleaned']\n",
        "    if text_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de texto '{text_column}' não encontrada no dataset.\")\n",
        "    elif sentiment_column not in new_data.columns:\n",
        "        print(f\"Erro: Coluna de sentimento '{sentiment_column}' não encontrada no dataset.\")\n",
        "    else:\n",
        "        X_new = new_data[text_column]\n",
        "        y_new = new_data[sentiment_column].map(sentiment_mapping)\n",
        "\n",
        "        # Certifique-se de que y_new está no formato numérico esperado (inteiros)\n",
        "        # Pode ser necessário aplicar o mesmo mapeamento que foi usado no treino\n",
        "        # Exemplo (se sentiment_mapping mapeia strings para ints):\n",
        "        # y_new = y_new.map({v: k for k, v in sentiment_mapping.items()})\n",
        "        # Certifique-se de que os rótulos numéricos correspondem aos usados no treino.\n",
        "        # Se y_new já vier como inteiros 0, 1, 2, essa linha acima não é necessária.\n",
        "        print(f\"Textos extraídos ({len(X_new)}). Rótulos extraídos ({len(y_new)}).\")\n",
        "        print(\"Primeiros 5 rótulos do novo dataset:\", y_new.head().tolist())\n",
        "\n",
        "\n",
        "        # 3. Gerar embeddings para o novo dataset\n",
        "        print(\"Gerando embeddings para o novo dataset...\")\n",
        "\n",
        "        # É crucial usar a mesma instância do embedder já carregado\n",
        "        # A sua classe Model2VecEmbeddings tem o método embed_documents para listas,\n",
        "        # que é mais eficiente do que chamar embed_query em loop.\n",
        "        if 'embedding_model_instance' in globals() and embedding_model_instance is not None:\n",
        "             X_new_vec_list = embedding_model_instance.embed_documents(X_new.tolist())\n",
        "             X_new_vec = np.array(X_new_vec_list)\n",
        "             print(f\"Embeddings para o novo dataset gerados: {X_new_vec.shape}\")\n",
        "\n",
        "             # 4. Avaliar o best_model no novo conjunto de embeddings\n",
        "             print(\"\\nAvaliando o melhor modelo no novo dataset...\")\n",
        "\n",
        "             if 'best_model' in globals() and best_model is not None:\n",
        "                 # Fazer as predições usando o modelo treinado\n",
        "                 y_pred_new_proba = best_model.predict(X_new_vec)\n",
        "                 y_pred_new = np.argmax(y_pred_new_proba, axis=-1) # Converter predições para rótulos\n",
        "\n",
        "                 # 5. Exibir métricas de avaliação\n",
        "                 print(\"\\nRelatório de Avaliação no Novo Dataset:\")\n",
        "                 # Use y_new e y_pred_new\n",
        "                 test_accuracy_new = accuracy_score(y_new, y_pred_new)\n",
        "                 test_f1_new = f1_score(y_new, y_pred_new, average='weighted')\n",
        "                 recall_negativo_new = recall_score(y_new, y_pred_new, average=None, labels=[0])[0] # Recall para a classe 0 (Negativo)\n",
        "\n",
        "                 print(f\"Acurácia no novo dataset: {test_accuracy_new:.4f}\")\n",
        "                 print(f\"F1-Score no novo dataset: {test_f1_new:.4f}\")\n",
        "                 print(f\"Recall para classe 'Negative' (0) no novo dataset: {recall_negativo_new:.4f}\")\n",
        "\n",
        "                 print(\"\\nClassification Report no Novo Dataset:\")\n",
        "                 # Assegure-se de que sentiment_mapping está definido e correto para target_names\n",
        "                 if 'sentiment_mapping' in globals():\n",
        "                     target_names_list = [str(sentiment_mapping[i]) for i in sorted(sentiment_mapping.keys())]\n",
        "                     print(classification_report(y_new, y_pred_new, target_names=target_names_list))\n",
        "                 else:\n",
        "                      print(classification_report(y_new, y_pred_new)) # Sem nomes se mapping não disponível\n",
        "\n",
        "                 # Opcional: Análise de Falsos Negativos no NOVO dataset\n",
        "                 print(\"\\nAnálise de Falsos Negativos no Novo Dataset:\")\n",
        "                 # Encontrar os índices onde o rótulo real é 0 (Negative) mas a predição não é 0\n",
        "                 false_negatives_indices_new = new_data.index[(y_new == 0) & (y_pred_new != 0)]\n",
        "\n",
        "                 if len(false_negatives_indices_new) > 0:\n",
        "                     print(f\"Encontrados {len(false_negatives_indices_new)} falsos negativos no novo dataset.\")\n",
        "                     print(\"Exemplos de falsos negativos (primeiros 5):\")\n",
        "                     # Use .loc com os índices do dataframe original para pegar os textos e rótulos\n",
        "                     for i, idx in enumerate(false_negatives_indices_new[:5]):\n",
        "                         original_text = new_data.loc[idx, text_column]\n",
        "                         true_label = y_new.loc[idx]\n",
        "                         predicted_label = y_pred_new[new_data.index.get_loc(idx)] # Encontra a predição pelo índice original\n",
        "                         # Mapear rótulos numéricos para nomes se sentiment_mapping estiver disponível\n",
        "                         true_label_name = sentiment_mapping.get(true_label, str(true_label)) if 'sentiment_mapping' in globals() else str(true_label)\n",
        "                         predicted_label_name = sentiment_mapping.get(predicted_label, str(predicted_label)) if 'sentiment_mapping' in globals() else str(predicted_label)\n",
        "\n",
        "                         print(f\"  {i+1}. Texto: '{original_text}'\")\n",
        "                         print(f\"     Real: {true_label_name} ({true_label}), Predito: {predicted_label_name} ({predicted_label})\")\n",
        "                 else:\n",
        "                     print(\"Nenhum falso negativo encontrado no novo dataset.\")\n",
        "\n",
        "             else:\n",
        "                 print(\"Erro: A variável 'best_model' não foi encontrada ou é None. Certifique-se de que o treinamento e a seleção do modelo foram executados antes da avaliação.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: Arquivo '{new_data_path}' não encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante o processamento do novo dataset: {e}\")\n",
        "\n",
        "print(\"\\n--- Fim da avaliação no novo dataset ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nyFGZXRjUxk",
        "outputId": "ad36e79e-677d-4dd8-d6ed-6ff65dbc2a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vetorizando textos...\n",
            "out\n",
            "\n",
            "Configurando Grid Search...\n",
            "\n",
            "Iniciando Grid Search...\n",
            "\n",
            "Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=200, activation=relu, regularizer=None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Overall Accuracy: 0.6837\n",
            "Recall (Negativo): 0.6522\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
            "Overall Accuracy: 0.9795\n",
            "Recall (Negativo): 0.9857\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "Overall Accuracy: 1.0000\n",
            "Recall (Negativo): 1.0000\n",
            "Overall Accuracy médio (validação cruzada): 0.8877\n",
            "Recall (Negativo) médio (validação cruzada): 0.8793\n",
            "------------------------------\n",
            "\n",
            "Tempo total de execução: 1.37 minutos\n",
            "\n",
            "Melhores parâmetros encontrados:\n",
            "units1: 128\n",
            "units2: 32\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.001\n",
            "optimizer: adam\n",
            "batch_size: 32\n",
            "epochs: 200\n",
            "activation: relu\n",
            "regularizer: None\n",
            "\n",
            "Melhor score de validação: 0.8793\n",
            "\n",
            "Avaliando no conjunto de teste...\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "\n",
            "Acurácia no teste: 0.7755\n",
            "F1-Score no teste: 0.7717\n",
            "\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.79      0.65      0.72        52\n",
            "     Neutral       0.76      0.74      0.75        35\n",
            "    Positive       0.77      0.90      0.83        60\n",
            "\n",
            "    accuracy                           0.78       147\n",
            "   macro avg       0.78      0.77      0.77       147\n",
            "weighted avg       0.78      0.78      0.77       147\n",
            "\n",
            "\n",
            "Métricas para classe Negative:\n",
            "Acurácia para Negative: 0.6538\n",
            "F1-Score para Negative: 0.7907\n",
            "\n",
            "Exemplos de falsos negativos:\n",
            "1. ive asked a cart attendant for a lotus leaf wrapped rice and she replied back rice and just walked away .;\n",
            "2. i know , you were too busy showing off your vintage tee shirt and looking bored , but my agenda is i ' m here to eat and enjoy the company of friends , seeking a pleasant experience .;\n",
            "3. the kitchen however , is almost always slow .;\n",
            "4. i went there for lunch and it wasnot as good as i expected from the reviews i read .;\n",
            "5. but when we looked at the menu , there weren't a lot of choices , most of them were dumplings in the appetizer section .;\n",
            "\n",
            "--- Avaliando no dataset: /content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet ---\n",
            "Dataset '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet' carregado com sucesso. Shape: (199, 13)\n",
            "Textos extraídos (199). Rótulos extraídos (199).\n",
            "Primeiros 5 rótulos do novo dataset: [2, 0, 1, 2, 2]\n",
            "Gerando embeddings para o novo dataset...\n",
            "Embeddings para o novo dataset gerados: (199, 512)\n",
            "\n",
            "Avaliando o melhor modelo no novo dataset...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
            "\n",
            "Relatório de Avaliação no Novo Dataset:\n",
            "Acurácia no novo dataset: 0.6784\n",
            "F1-Score no novo dataset: 0.6930\n",
            "Recall para classe 'Negative' (0) no novo dataset: 0.7000\n",
            "\n",
            "Classification Report no Novo Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.70      0.62        50\n",
            "           1       0.25      0.32      0.28        25\n",
            "           2       0.88      0.74      0.80       124\n",
            "\n",
            "    accuracy                           0.68       199\n",
            "   macro avg       0.56      0.59      0.57       199\n",
            "weighted avg       0.72      0.68      0.69       199\n",
            "\n",
            "\n",
            "Análise de Falsos Negativos no Novo Dataset:\n",
            "Encontrados 15 falsos negativos no novo dataset.\n",
            "Exemplos de falsos negativos (primeiros 5):\n",
            "  1. Texto: 'knowledge of the chef and the waitress are below average .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  2. Texto: 'the bartender on my most recent visit was so incredibly rude that i will never go back .'\n",
            "     Real: 0 (0), Predito: 2 (2)\n",
            "  3. Texto: 'if you 're not familiar with the cuisine , you may encounter problems ordering here and be sure to bring plenty of cash with you as they don't accept credit cards .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  4. Texto: 'besides , when you have bad service , that 's less money you have to tip .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "  5. Texto: 'for those prices and the dressed up atmosphere you expect more and should get more .'\n",
            "     Real: 0 (0), Predito: 1 (1)\n",
            "\n",
            "--- Fim da avaliação no novo dataset ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold  # Adicionado StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder  # Importado LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import time\n",
        "from sklearn.metrics import make_scorer #importa o make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import spacy\n",
        "\n",
        "# 1. Carregar e preparar os dados\n",
        "print(\"Carregando dados...\")\n",
        "df_train = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet')\n",
        "\n",
        "if 'target' not in df_train.columns:\n",
        "    df_train['target'] = df_train['sentiment']\n",
        "\n",
        "# Codificar labels\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "y = df_train['target'].map(sentiment_mapping)\n",
        "X = df_train['comment_cleaned']\n",
        "\n",
        "# Split dos dados\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "\n",
        "# Carregar um modelo do SpaCy (escolha um que inclua embeddings)\n",
        "nlp = spacy.load(\"en_core_web_lg\")  # ou \"en_core_web_lg\" para embeddings maiores\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    doc = nlp(text)\n",
        "    return doc.vector\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [20],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores = []\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")"
      ],
      "metadata": {
        "id": "e7dZSGVn9SKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Remova a importação de train_test_split se ela só for usada aqui\n",
        "# from sklearn.model_selection import train_test_split\n",
        "import time # Mantenha se usado no restante do script\n",
        "from sklearn.model_selection import StratifiedKFold # Mantenha para a validação cruzada no treino\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, recall_score # Mantenha para avaliação\n",
        "# Importe suas classes e funções Keras/TensorFlow conforme necessário\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "# from tensorflow.keras.regularizers import l1, l2 # Importe se estiver usando regularização\n",
        "# Certifique-se de que Model2VecEmbeddings e StaticModel estão acessíveis/importados\n",
        "# from your_module import Model2VecEmbeddings, StaticModel\n",
        "from typing import List # Necessário para a definição da classe mock/real\n",
        "\n",
        "# --- Definição da sua classe Model2VecEmbeddings e StaticModel ---\n",
        "# Copie/cole ou importe suas definições reais aqui se não estiverem disponíveis globalmente.\n",
        "# Se já estiverem definidas/importadas, remova este bloco try/except com classes mock.\n",
        "try:\n",
        "    _ = Model2VecEmbeddings\n",
        "    _ = StaticModel\n",
        "except NameError:\n",
        "    print(\"Model2VecEmbeddings or StaticModel not found. Using mock classes for demonstration.\")\n",
        "    # --- Mock classes for demonstration if yours aren't available ---\n",
        "    class StaticModel:\n",
        "        def __init__(self, model_name):\n",
        "            print(f\"Mock Loading model: {model_name}\")\n",
        "            self.embedding_size = 768 # Tamanho típico de embedding\n",
        "\n",
        "        def encode(self, texts: List[str]):\n",
        "            # print(f\"Mock Encoding {len(texts)} texts...\")\n",
        "            return np.random.rand(len(texts), self.embedding_size).tolist()\n",
        "\n",
        "        @classmethod\n",
        "        def from_pretrained(cls, model_name):\n",
        "            return cls(model_name)\n",
        "\n",
        "    class Model2VecEmbeddings:\n",
        "        \"\"\"Wrapper para o Model2Vec como Embeddings - MOCK\"\"\"\n",
        "        def __init__(self, model_name: str = \"minishlab/potion-base-4M\", similarity_threshold: float = 0.85):\n",
        "             print(f\"Mock Instantiating Model2VecEmbeddings: {model_name}\")\n",
        "             self.model = StaticModel.from_pretrained(model_name)\n",
        "             self.similarity_threshold = similarity_threshold\n",
        "\n",
        "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "            return self.model.encode(texts).tolist()\n",
        "\n",
        "        def embed_query(self, text: str) -> List[float]:\n",
        "            return self.model.encode([text]).tolist()[0]\n",
        "    # --- Fim das classes mock ---\n",
        "# --- Fim da definição da classe (se necessário) ---\n",
        "\n",
        "\n",
        "# 1. Carregar e preparar os dados de TREINO (do primeiro arquivo)\n",
        "print(\"Carregando dados de treino do arquivo: /content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet\")\n",
        "df_train = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet')\n",
        "\n",
        "# Use 'target' ou 'sentiment' para os rótulos de treino\n",
        "if 'target' not in df_train.columns:\n",
        "    # Se 'target' não existe, use 'sentiment' como a coluna de rótulo\n",
        "    train_label_column = 'sentiment'\n",
        "    print(\"Coluna 'target' não encontrada no arquivo de treino. Usando 'sentiment' como coluna de rótulo.\")\n",
        "elif df_train['target'].dtype == 'object':\n",
        "     # Se 'target' existe e é string, use 'target'\n",
        "     train_label_column = 'target'\n",
        "     print(\"Usando coluna 'target' (string) como rótulo no arquivo de treino.\")\n",
        "else:\n",
        "    # Se 'target' existe e não é string (assumindo que já é int 0, 1, 2)\n",
        "    train_label_column = 'target'\n",
        "    print(\"Usando coluna 'target' (numérica) como rótulo no arquivo de treino.\")\n",
        "\n",
        "\n",
        "# Codificar labels (este mapeamento será usado para treino e teste)\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "\n",
        "# Aplicar o mapeamento para y_train\n",
        "# Se a coluna de rótulo já for numérica (0, 1, 2), .map pode gerar NaNs ou não mudar nada,\n",
        "# dependendo do conteúdo exato. É mais seguro verificar antes ou garantir que o mapeamento seja robusto.\n",
        "# Assumindo que os rótulos no arquivo de treino são strings ('Negative', 'Neutral', 'Positive'):\n",
        "y_train = df_train[train_label_column].map(sentiment_mapping)\n",
        "X_train = df_train['comment_cleaned'] # Coluna de texto no arquivo de treino\n",
        "\n",
        "# Verificação de rótulos de treino\n",
        "print(\"Verificando rótulos de treino (y_train)...\")\n",
        "print(f\"Tipo de dado de y_train: {y_train.dtype}\")\n",
        "print(f\"Valores únicos em y_train: {y_train.unique()}\")\n",
        "if y_train.isnull().any():\n",
        "    print(\"Aviso: NaNs encontrados em y_train após o mapeamento. Verifique os rótulos originais no dataset de treino.\")\n",
        "\n",
        "\n",
        "# 1. Carregar e preparar os dados de TESTE FINAL (do novo arquivo)\n",
        "new_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet'\n",
        "print(f\"\\nCarregando dados de teste final do arquivo: {new_data_path}\")\n",
        "df_test_final = pd.read_parquet(new_data_path)\n",
        "\n",
        "# Definir as colunas de texto e rótulo para o arquivo de teste.\n",
        "# Ajuste se os nomes das colunas neste arquivo forem diferentes.\n",
        "text_column_test = 'comment_cleaned'\n",
        "sentiment_column_test = 'sentiment'\n",
        "\n",
        "if text_column_test not in df_test_final.columns:\n",
        "    print(f\"Erro: Coluna de texto '{text_column_test}' não encontrada no dataset de teste final.\")\n",
        "    raise ValueError(f\"Missing text column '{text_column_test}' in final test data.\")\n",
        "if sentiment_column_test not in df_test_final.columns:\n",
        "     print(f\"Erro: Coluna de sentimento '{sentiment_column_test}' não encontrada no dataset de teste final.\")\n",
        "     raise ValueError(f\"Missing sentiment column '{sentiment_column_test}' in final test data.\")\n",
        "\n",
        "X_test_final = df_test_final[text_column_test]\n",
        "y_test_final = df_test_final[sentiment_column_test]\n",
        "\n",
        "# --- IMPORTANTE: GARANTIR QUE y_test_final ESTEJA NO FORMATO NUMÉRICO CORRETO (0, 1, 2) ---\n",
        "# Aplique o mapeamento de rótulos para y_test_final usando o MESMO sentiment_mapping\n",
        "# Se y_test_final já for numérico (int 0, 1, 2), a linha .map(inverse_sentiment_mapping)\n",
        "# abaixo deve ser ajustada ou removida.\n",
        "# Vamos verificar o tipo de dado e aplicar o mapeamento SE for string.\n",
        "\n",
        "print(\"Verificando rótulos de teste final (y_test_final) do novo dataset...\")\n",
        "print(f\"Tipo de dado de y_test_final: {y_test_final.dtype}\")\n",
        "print(f\"Valores únicos em y_test_final: {y_test_final.unique()}\")\n",
        "\n",
        "# Codificar labels\n",
        "sentiment_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "y_test_final = df_test_final[sentiment_column_test].map(sentiment_mapping)\n",
        "\n",
        "\n",
        "# Neste ponto:\n",
        "# X_train contém os textos do primeiro arquivo (para treino/VC)\n",
        "# y_train contém os rótulos numéricos (0, 1, 2) do primeiro arquivo (para treino/VC)\n",
        "# X_test_final contém os textos do segundo arquivo (para teste FINAL)\n",
        "# y_test_final contém os rótulos numéricos (0, 1, 2) do segundo arquivo (para teste FINAL)\n",
        "# sentiment_mapping mapeia 0, 1, 2 para 'Negative', 'Neutral', 'Positive' (para relatórios)\n",
        "# df_test_final contém o dataframe completo do arquivo de teste final (útil para exemplos de erro)\n",
        "\n",
        "\n",
        "# A variável X original (referenciando o texto completo do treino)\n",
        "# pode ser mantida se outras partes do seu código a utilizarem,\n",
        "# mas para mostrar exemplos de erro no conjunto de teste FINAL,\n",
        "# você precisará usar df_test_final ou X_test_final.\n",
        "\n",
        "# Para fins de compatibilidade com o seu código anterior que usava `X.iloc[index]`\n",
        "# para mostrar exemplos de erro (que presumia que X continha todos os dados antes do split),\n",
        "# vamos criar uma nova variável para o dataframe de treino completo se necessário,\n",
        "# mas o ideal é adaptar a parte de erro para usar `df_test_final`\n",
        "# já que você mostrará erros do *conjunto de teste final*.\n",
        "\n",
        "# Removida a linha de train_test_split.\n",
        "# As variáveis X_test e y_test do split aleatório NÃO SÃO MAIS USADAS.\n",
        "# O restante do script precisará usar X_test_final e y_test_final\n",
        "# na seção de avaliação final.\n",
        "\n",
        "print(\"\\nPreparação de dados concluída.\")\n",
        "print(f\"Tamanho do conjunto de treino: {len(X_train)} exemplos.\")\n",
        "print(f\"Tamanho do conjunto de teste final: {len(X_test_final)} exemplos.\")\n",
        "\n",
        "\n",
        "# --- O restante do seu código deve continuar a partir daqui ---\n",
        "# O processo de vetorização/embedding de X_train para criar X_train_vec\n",
        "# O processo de Grid Search/Validação Cruzada usando X_train_vec e y_train\n",
        "# O treino do best_model final usando X_train_vec e y_train\n",
        "# E então, a avaliação final usará X_test_final (para gerar X_test_final_vec) e y_test_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LzjHzoj9_Cz",
        "outputId": "8c047b2a-1d6c-4aec-92f6-6d1db5c8961a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dados de treino do arquivo: /content/drive/MyDrive/Colab Notebooks/dataset_train_trim_synthetic_balanced.parquet\n",
            "Coluna 'target' não encontrada no arquivo de treino. Usando 'sentiment' como coluna de rótulo.\n",
            "Verificando rótulos de treino (y_train)...\n",
            "Tipo de dado de y_train: int64\n",
            "Valores únicos em y_train: [0 1 2]\n",
            "\n",
            "Carregando dados de teste final do arquivo: /content/drive/MyDrive/Colab Notebooks/dataset_valid_with_sentiment_fix_negative.parquet\n",
            "Verificando rótulos de teste final (y_test_final) do novo dataset...\n",
            "Tipo de dado de y_test_final: object\n",
            "Valores únicos em y_test_final: ['Positive' 'Negative' 'Neutral']\n",
            "\n",
            "Preparação de dados concluída.\n",
            "Tamanho do conjunto de treino: 733 exemplos.\n",
            "Tamanho do conjunto de teste final: 199 exemplos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vetorização do texto\n",
        "print(\"Vetorizando textos...\")\n",
        "# vectorizer = TfidfVectorizer(\n",
        "#     max_features=8000,\n",
        "#     ngram_range=(1, 2),\n",
        "#     stop_words='english'\n",
        "# )\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Converter para arrays densos\n",
        "# X_train_vec = X_train_vec.toarray()\n",
        "# X_test_vec = X_test_vec.toarray()\n",
        "\n",
        "\n",
        "embedding_model_instance = Model2VecEmbeddings(model_name=\"minishlab/potion-base-32M\")\n",
        "\n",
        "def get_mean_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates embedding for a single text using the pre-loaded Model2VecEmbeddings instance.\n",
        "    Note: This function now uses the globally instantiated 'embedding_model_instance'.\n",
        "    The name 'get_mean_embedding' might not be strictly accurate for all models,\n",
        "    as it's now getting a single vector per text, not necessarily a mean of token vectors.\n",
        "    \"\"\"\n",
        "    # Usa a instância do Model2VecEmbeddings e chama o método para um único texto\n",
        "    # embed_query retorna uma lista de floats [float, float, ...]\n",
        "    return embedding_model_instance.embed_query(text)\n",
        "\n",
        "X_train_vec = np.array([get_mean_embedding(text) for text in X_train])\n",
        "print(f\"Length X_train_vec: {len(X_train_vec)}\")\n",
        "X_test_vec = np.array([get_mean_embedding(text) for text in X_test])\n",
        "\n",
        "print(f\"Length X_test_vec: {len(X_test_vec)}\")\n",
        "\n",
        "# 3. Função para criar modelo\n",
        "def create_model(units1=64, units2=32, dropout_rate=0.3,\n",
        "                learning_rate=0.001, optimizer='adam',\n",
        "                activation='relu', regularizer=None):\n",
        "    model = Sequential()\n",
        "    # Exemplo de matriz de custos (penalizando mais falsos positivos para a classe 0 - Negativo)\n",
        "    class_weights = {0: 5, 1: 1, 2: 1} # Peso 5 para a classe Negativo, 1 para as outras\n",
        "    # Camada de entrada\n",
        "    model.add(Dense(\n",
        "        units=units1,\n",
        "        input_dim=X_train_vec.shape[1],\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada oculta\n",
        "    model.add(Dense(\n",
        "        units=units2,\n",
        "        activation=activation,\n",
        "        kernel_regularizer=regularizer\n",
        "    ))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Camada de saída\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Compilar modelo\n",
        "    if optimizer == 'adam':\n",
        "        opt = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        opt = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # Importante: para labels inteiros\n",
        "        optimizer=opt,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "#Testando: units1=128, units2=32, dropout=0.2, lr=0.001, opt=adam, batch=32, epochs=20, activation=relu, regularizer=None\n",
        "\n",
        "# 4. Configuração da Grid Search\n",
        "print(\"\\nConfigurando Grid Search...\")\n",
        "param_grid = {\n",
        "    'units1': [128],\n",
        "    'units2': [32],\n",
        "    'dropout_rate': [0.2],\n",
        "    'learning_rate': [0.001],\n",
        "    'optimizer': ['adam'],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [20],\n",
        "    'activation': ['relu'],\n",
        "    'regularizer': [None]\n",
        "}\n",
        "\n",
        "# 5. Grid Search Manual\n",
        "print(\"\\nIniciando Grid Search...\")\n",
        "start_time = time.time()\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "print(f\"Length y_train {len(y_train)}\")\n",
        "# Para cada combinação de parâmetros\n",
        "for units1 in param_grid['units1']:\n",
        "    for units2 in param_grid['units2']:\n",
        "        for dropout_rate in param_grid['dropout_rate']:\n",
        "            for learning_rate in param_grid['learning_rate']:\n",
        "                for optimizer_name in param_grid['optimizer']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "                            for activation in param_grid['activation']:\n",
        "                                for regularizer_type in param_grid['regularizer']:\n",
        "                                    print(f\"\\nTestando: units1={units1}, units2={units2}, dropout={dropout_rate}, lr={learning_rate}, opt={optimizer_name}, batch={batch_size}, epochs={epochs}, activation={activation}, regularizer={regularizer_type}\")\n",
        "\n",
        "                                    # Criar o modelo com os parâmetros atuais\n",
        "                                    model = create_model(\n",
        "                                        units1=units1,\n",
        "                                        units2=units2,\n",
        "                                        dropout_rate=dropout_rate,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        optimizer=optimizer_name,\n",
        "                                        activation=activation,\n",
        "                                        regularizer=regularizer_type\n",
        "                                    )\n",
        "\n",
        "                                    # Validação cruzada (cross-validation) - StratifiedKFold para manter a proporção das classes\n",
        "                                    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # Mantém a proporção das classes\n",
        "                                    scores = []\n",
        "                                    for train_index, val_index in kfold.split(X_train_vec, y_train):\n",
        "                                        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
        "                                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usando .iloc para indexar corretamente\n",
        "                                        # Treinar o modelo no conjunto de treinamento da dobra (fold)\n",
        "                                        model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=0) # verbose=0 silencia a saída de treinamento\n",
        "\n",
        "                                        # Fazer a predição no conjunto de validação da dobra\n",
        "                                        y_pred_fold = np.argmax(model.predict(X_val_fold), axis=-1)  # Convertendo as predições de probabilidade para rótulos de classe\n",
        "                                        # Calcular o F1-score ponderado para a dobra\n",
        "                                        # f1 = f1_score(y_val_fold, y_pred_fold, average='weighted')\n",
        "                                        # scores.append(f1)\n",
        "\n",
        "                                        recall_negativo = recall_score(y_val_fold, y_pred_fold, average=None, labels=[0])[0]  # labels=[0] para o recall da classe 0 (Negativo)\n",
        "                                        print(f\"Recall (Negativo): {recall_negativo:.4f}\")\n",
        "                                        scores.append(recall_negativo)\n",
        "\n",
        "                                    # Calcular a média dos scores de validação cruzada\n",
        "                                    # mean_f1 = np.mean(scores)\n",
        "                                    # print(f\"F1-score médio (validação cruzada): {mean_f1:.4f}\")\n",
        "\n",
        "                                    mean_recall_negativo = np.mean(scores)\n",
        "                                    print(f\"Recall (Negativo) médio (validação cruzada): {mean_recall_negativo:.4f}\")\n",
        "\n",
        "\n",
        "                                    # Se o score for melhor, salvar os parâmetros e o modelo\n",
        "                                    if mean_recall_negativo > best_score:\n",
        "                                        best_score = mean_recall_negativo\n",
        "                                        best_params = {\n",
        "                                            'units1': units1,\n",
        "                                            'units2': units2,\n",
        "                                            'dropout_rate': dropout_rate,\n",
        "                                            'learning_rate': learning_rate,\n",
        "                                            'optimizer': optimizer_name,\n",
        "                                            'batch_size': batch_size,\n",
        "                                            'epochs': epochs,\n",
        "                                            'activation': activation,\n",
        "                                            'regularizer': regularizer_type\n",
        "                                        }\n",
        "                                        best_model = model # Salvando o modelo treinado com os melhores parâmetros\n",
        "                                    print(\"-\" * 30)  # Separador visual\n",
        "\n",
        "\n",
        "print(f\"\\nTempo total de execução: {(time.time() - start_time) / 60:.2f} minutos\")\n",
        "\n",
        "# 7. Resultados\n",
        "print(\"\\nMelhores parâmetros encontrados:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "print(f\"\\nMelhor score de validação: {best_score:.4f}\")\n",
        "\n",
        "# 8. Avaliação no conjunto de teste (usando o melhor modelo treinado)\n",
        "print(\"\\nAvaliando no conjunto de teste...\")\n",
        "\n",
        "if best_model is not None:  # Certifique-se de que um modelo foi treinado\n",
        "    y_pred = np.argmax(best_model.predict(X_test_vec), axis=-1) # Convertendo as predições de probabilidade para rótulos de classe\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    print(f\"\\nAcurácia no teste: {test_accuracy:.4f}\")\n",
        "    print(f\"F1-Score no teste: {test_f1:.4f}\")\n",
        "\n",
        "    # 9. Relatório detalhado\n",
        "    print(\"\\nRelatório de Classificação:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=sentiment_mapping.keys()))\n",
        "\n",
        "    # 10. Análise por classe (especialmente para negativas)\n",
        "    print(\"\\nMétricas para classe Negative:\")\n",
        "    neg_mask = (y_test == 0)\n",
        "    if np.any(neg_mask):  # Garante que haja exemplos da classe negativa\n",
        "        neg_accuracy = accuracy_score(y_test[neg_mask], y_pred[neg_mask])\n",
        "        neg_f1 = f1_score(y_test[neg_mask], y_pred[neg_mask], average='weighted')\n",
        "\n",
        "        print(f\"Acurácia para Negative: {neg_accuracy:.4f}\")\n",
        "        print(f\"F1-Score para Negative: {neg_f1:.4f}\")\n",
        "    else:\n",
        "        print(\"Nenhum exemplo da classe Negative encontrado no conjunto de teste.\")\n",
        "\n",
        "    # 11. Exemplos de erros (falsos negativos)\n",
        "    false_negatives = X_test[ (y_test == 0) & (y_pred != 0) ]  # Corrigido para usar os índices originais\n",
        "    if len(false_negatives) > 0:\n",
        "        print(\"\\nExemplos de falsos negativos:\")\n",
        "        #Recupera o texto original usando os indices originais.\n",
        "        false_negatives_indices = np.where((y_test == 0) & (y_pred != 0))[0] # Obtem os indices dos falsos negativos.\n",
        "        for i, index in enumerate(false_negatives_indices[:5]): # Usa os indices para acessar X_test\n",
        "            print(f\"{i+1}. {X.iloc[index]}\") # Usa X.iloc para pegar os textos originais.\n",
        "    else:\n",
        "        print(\"Nenhum falso negativo encontrado.\")\n",
        "else:\n",
        "    print(\"Nenhum modelo foi treinado (nenhuma combinação de parâmetros foi melhor).\")\n",
        "\n",
        "# Salvar o melhor modelo\n",
        "if best_model is not None:\n",
        "    # Criar um diretório para salvar o modelo (opcional)\n",
        "    import os\n",
        "    model_dir = \"saved_models\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Definir o nome do arquivo do modelo\n",
        "    model_filename = os.path.join(model_dir, \"best_sentiment_model.h5\")\n",
        "\n",
        "    # Salvar o modelo\n",
        "    best_model.save(model_filename)\n",
        "    print(f\"\\nMelhor modelo salvo em: {model_filename}\")\n",
        "\n",
        "    # Se você também quiser salvar o vetorizador (embora no seu caso está usando embeddings)\n",
        "    # Você pode querer salvar informações sobre o embedding_model_instance\n",
        "    # Isso pode ser mais complexo dependendo da implementação do Model2VecEmbeddings\n",
        "else:\n",
        "    print(\"Nenhum modelo para salvar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "YTa8TfkCAswy",
        "outputId": "5e299da2-b74b-490a-b558-1866e15dbfa7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vetorizando textos...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Model2VecEmbeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3fa6e0c2d880>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0membedding_model_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel2VecEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minishlab/potion-base-32M\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mean_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Model2VecEmbeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49uQ5qi4xQYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}