{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8c8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from model2vec import StaticModel\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9945aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2VecEmbeddings(Embeddings):\n",
    "        \"\"\"Wrapper para o Model2Vec como Embeddings do LangChain\"\"\"\n",
    "        def __init__(self, model_name: str):\n",
    "            self.model = StaticModel.from_pretrained(model_name)\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return self.model.encode(texts).tolist()\n",
    "        \n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            return self.model.encode([text]).tolist()[0]\n",
    "        \n",
    "class DeduplicationProcessor:\n",
    "    def __init__(self, model_name: str = \"minishlab/potion-base-2M\", similarity_threshold: int = 0.85):\n",
    "        \"\"\"Inicializa o processador com o modelo de embeddings\"\"\"\n",
    "        self.embedder = Model2VecEmbeddings(model_name)\n",
    "        self.similarity_threshold = similarity_threshold  # Ajuste conforme necessário\n",
    "        self.min_length = 10  # Comentários muito curtos serão ignorados    \n",
    "\n",
    "    def load_data(self, parquet_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Carrega os dados do arquivo parquet\"\"\"\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "        \n",
    "        # Pré-filtro básico\n",
    "        df = df.dropna(subset=['comment_cleaned'])\n",
    "        df = df[df['comment_cleaned'].str.len() >= self.min_length]\n",
    "        \n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Gera embeddings para todos os textos\"\"\"\n",
    "        print(\"Gerando embeddings...\")\n",
    "        return np.array(self.embedder.embed_documents(texts))\n",
    "\n",
    "    def find_similar_pairs(self, embeddings: np.ndarray) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Identifica pares similares usando similaridade de cosseno\"\"\"\n",
    "        print(\"Calculando similaridades...\")\n",
    "        sim_matrix = cosine_similarity(embeddings)\n",
    "        np.fill_diagonal(sim_matrix, 0)  # Ignora auto-similaridade\n",
    "        \n",
    "        similar_pairs = []\n",
    "        n = sim_matrix.shape[0]\n",
    "        \n",
    "        # Encontra pares acima do threshold\n",
    "        for i in tqdm(range(n), desc=\"Processando similaridades\"):\n",
    "            for j in range(i+1, n):\n",
    "                if sim_matrix[i, j] > self.similarity_threshold:\n",
    "                    similar_pairs.append((i, j))\n",
    "        \n",
    "        return similar_pairs\n",
    "\n",
    "    def cluster_similar_comments(self, df: pd.DataFrame, similar_pairs: List[Tuple[int, int]]) -> pd.DataFrame:\n",
    "        \"\"\"Agrupa comentários similares e mantém apenas um representante\"\"\"\n",
    "        print(\"Agrupando comentários similares...\")\n",
    "        clusters = []\n",
    "        visited = set()\n",
    "        \n",
    "        # Cria clusters de similaridade\n",
    "        for i, j in similar_pairs:\n",
    "            if i not in visited and j not in visited:\n",
    "                clusters.append({i, j})\n",
    "                visited.update({i, j})\n",
    "            elif i in visited and j not in visited:\n",
    "                for cluster in clusters:\n",
    "                    if i in cluster:\n",
    "                        cluster.add(j)\n",
    "                        visited.add(j)\n",
    "                        break\n",
    "            elif j in visited and i not in visited:\n",
    "                for cluster in clusters:\n",
    "                    if j in cluster:\n",
    "                        cluster.add(i)\n",
    "                        visited.add(i)\n",
    "                        break\n",
    "        \n",
    "        # Seleciona representantes (o comentário mais longo de cada cluster)\n",
    "        to_remove = set()\n",
    "        for cluster in clusters:\n",
    "            cluster_texts = [(idx, df.loc[idx, 'comment_cleaned']) for idx in cluster]\n",
    "            # Ordena por comprimento e seleciona o mais longo\n",
    "            cluster_texts.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "            representative = cluster_texts[0][0]\n",
    "            to_remove.update([idx for idx, _ in cluster_texts[1:]])\n",
    "        \n",
    "        # Cria coluna de hash para identificação de duplicatas exatas\n",
    "        df['text_hash'] = df['comment_cleaned'].apply(\n",
    "            lambda x: hashlib.md5(x.strip().lower().encode()).hexdigest()\n",
    "        )\n",
    "        \n",
    "        # Remove duplicatas exatas primeiro\n",
    "        df = df.drop_duplicates(subset=['text_hash'], keep='first')\n",
    "        \n",
    "        # Remove comentários similares não representativos\n",
    "        df = df.drop(index=list(to_remove)).reset_index(drop=True)\n",
    "        \n",
    "        return df.drop(columns=['text_hash'])\n",
    "\n",
    "    def process(self, input_path: str, output_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Pipeline completo de processamento\"\"\"\n",
    "        # 1. Carregar dados\n",
    "        df = self.load_data(input_path)\n",
    "        print(f\"Total de comentários inicial: {len(df)}\")\n",
    "        \n",
    "        # 2. Gerar embeddings\n",
    "        embeddings = self.generate_embeddings(df['comment_cleaned'].tolist())\n",
    "        \n",
    "        # 3. Encontrar pares similares\n",
    "        similar_pairs = self.find_similar_pairs(embeddings)\n",
    "        print(f\"Pares similares encontrados: {len(similar_pairs)}\")\n",
    "        \n",
    "        # 4. Clusterizar e remover duplicatas\n",
    "        df_dedup = self.cluster_similar_comments(df, similar_pairs)\n",
    "        print(f\"Total de comentários após deduplicação: {len(df_dedup)}\")\n",
    "        print(f\"Comentários removidos: {len(df) - len(df_dedup)}\")\n",
    "        \n",
    "        # 5. Salvar resultados\n",
    "        df_dedup.to_parquet(output_path, index=False)\n",
    "        print(f\"Dados processados salvos em {output_path}\")\n",
    "        \n",
    "        return (df_dedup, similar_pairs, df, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65225981",
   "metadata": {},
   "source": [
    "INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f10d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de comentários inicial: 341\n",
      "Gerando embeddings...\n",
      "Calculando similaridades...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando similaridades: 100%|██████████| 341/341 [00:00<00:00, 24997.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares similares encontrados: 45\n",
      "Agrupando comentários similares...\n",
      "Total de comentários após deduplicação: 298\n",
      "Comentários removidos: 43\n",
      "Dados processados salvos em ../tests\\dataset_train_with_sentiment_positives_to_trim.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processor = DeduplicationProcessor(similarity_threshold=0.7)\n",
    "    \n",
    "    # Arquivos de entrada e saída\n",
    "    input_parquet = \"../tests\\dataset_train_with_sentiment_positives_to_trim.parquet\"\n",
    "    output_parquet = \"../tests\\dataset_train_with_sentiment_positives_to_trim.parquet\"\n",
    "    \n",
    "    # Executar pipeline\n",
    "    (df_dedup, similar_pairs, df, embeddings) = processor.process(input_parquet, output_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bcb5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_comments(df: pd.DataFrame, \n",
    "                        similar_pairs: List[Tuple[int, int]], \n",
    "                        embeddings: np.ndarray,\n",
    "                        top_n: int = 20000) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retorna uma lista dos comentários similares com seus textos e scores de similaridade.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com os comentários (deve ter coluna 'comment_cleaned')\n",
    "        similar_pairs: Lista de tuplas com índices dos pares similares\n",
    "        embeddings: Matriz de embeddings dos comentários\n",
    "        top_n: Quantos pares similares retornar (ordenados por similaridade)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de dicionários com pares similares e informações\n",
    "    \"\"\"\n",
    "    # Calcula similaridades para todos os pares\n",
    "    similar_comments = []\n",
    "    for i, j in similar_pairs:\n",
    "        sim_score = cosine_similarity(\n",
    "            embeddings[i].reshape(1, -1), \n",
    "            embeddings[j].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similar_comments.append({\n",
    "            'index1': i,\n",
    "            'index2': j,\n",
    "            'text1': df.loc[i, 'comment_cleaned'],\n",
    "            'text2': df.loc[j, 'comment_cleaned'],\n",
    "            'similarity': sim_score,\n",
    "            'length_diff': abs(len(df.loc[i, 'comment_cleaned']) - len(df.loc[j, 'comment_cleaned']))\n",
    "        })\n",
    "    \n",
    "    # Ordena por similaridade decrescente\n",
    "    similar_comments.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    # Retorna apenas os top_n mais similares\n",
    "    return similar_comments[:top_n]\n",
    "\n",
    "# Função para visualizar os resultados de forma elegante\n",
    "def print_similar_pairs(similar_comments: List[Dict]):\n",
    "    \"\"\"Imprime os pares similares de forma formatada\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{'PAIR':<6} | {'SIMILARITY':<10} | {'TEXT 1':<50} | {'TEXT 2':<50}\")\n",
    "    print(f\"{'-'*120}\")\n",
    "    for i, pair in enumerate(similar_comments, 1):\n",
    "        text1_short = (pair['text1'][:47] + '...') if len(pair['text1']) > 50 else pair['text1']\n",
    "        text2_short = (pair['text2'][:47] + '...') if len(pair['text2']) > 50 else pair['text2']\n",
    "        print(f\"{i:<6} | {pair['similarity']:.4f}    | {text1_short:<50} | {text2_short:<50}\")\n",
    "    print(f\"{'='*120}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb202f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = get_similar_comments(df=df, similar_pairs=similar_pairs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df668887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PAIR   | SIMILARITY | TEXT 1                                             | TEXT 2                                            \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "1      | 0.8942    | the atmosphere is great ! ! !                      | the design and atmosphere is just as good .       \n",
      "2      | 0.8902    | the pizza was really good .                        | the pizza was pretty good and huge .              \n",
      "3      | 0.8628    | the pizza was really good .                        | great pizza and fantastic service .               \n",
      "4      | 0.8572    | ballato 's is consistently delicious authentic ... | the food is authentic italian - delicious !       \n",
      "========================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_similar_pairs(comments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
